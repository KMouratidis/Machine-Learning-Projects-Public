{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithms, Part 9, Neural Networks\n",
    "\n",
    "Artificial Neural Networks were initially by our understang of biological Neural Networks within the human brain, and have since their (re-)discovery revolutionized Machine Learning (for their differences, take a look at [our previous article](https://www.theaispace.com/blog/difference-between-ai-machine-learning-and-deep-learning)). We will not talk about how optimization works (e.g. gradient descent), the types of layers, weights, matrix multiplications; these would assume some basic yet solid math knowledge, so we skip them.\n",
    "\n",
    "There are many libraries and frameworks that help you write your own Neural Networks, and we already [discussed some of them](https://www.theaispace.com/blog/the-most-important-python-libraries-used-for-ai-and-machine-learning) before. Here we will use sklearn's MLP (Multi-Layer Perceptron, a very basic Neural Network) and we will leave Tensorflow (and its high-level interface: Keras - which we will demo here) for a future article. We will once again focus on the digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konstantinos\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier # This is an ensemble model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we will work with the real MNIST dataset, and get it from keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras took care of splitting the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using sklearn here, we need to flatten the X data, and we'll just use a 6th of our training data and a 5th of our test data (out of speed concerns). Feel free to use all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((-1, 28*28))[:10000]\n",
    "x_test = x_test.reshape((-1, 28*28))[:2000]\n",
    "\n",
    "y_train = y_train[:10000]\n",
    "y_test = y_test[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNet = MLPClassifier()\n",
    "\n",
    "NNet.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 99.32%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {:.2f}%\".format(NNet.score(x_train, y_train)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 88.20%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy: {:.2f}%\".format(NNet.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with a LineraSVC (chosen over SVC due to performance) from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 96.01%\n",
      "Test set accuracy: 81.65%\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(\"Training set accuracy: {:.2f}%\".format(clf.score(x_train, y_train)*100))\n",
    "print(\"Test set accuracy: {:.2f}%\".format(clf.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were quite better (~7% in this particular run) for the MLP, even without us playing around with the parameters. Let's try just that. For convenience, we wrap the four steps in a function with optional keyword arguments (`**params`). This is just a shortcut so we can pass whatever parameters we want (if you wonder what are the parameters you can specify, you can always [check the documentation on MLP](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier), or any other algorithm for that matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neural_net(**params):\n",
    "    NNet = MLPClassifier(**params)\n",
    "    NNet.fit(x_train, y_train)\n",
    "    print(\"Training set accuracy: {:.2f}%\".format(NNet.score(x_train, y_train)*100))\n",
    "    print(\"Test set accuracy: {:.2f}%\".format(NNet.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 95.72%\n",
      "Test set accuracy: 90.85%\n"
     ]
    }
   ],
   "source": [
    "fit_neural_net(activation=\"logistic\", solver='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 91.19%\n",
      "Test set accuracy: 86.30%\n"
     ]
    }
   ],
   "source": [
    "fit_neural_net(activation=\"tanh\", solver='adam', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 99.80%\n",
      "Test set accuracy: 88.15%\n"
     ]
    }
   ],
   "source": [
    "fit_neural_net(activation=\"relu\", solver='adam', learning_rate=\"adaptive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 99.05%\n",
      "Test set accuracy: 91.55%\n"
     ]
    }
   ],
   "source": [
    "# Note that this will take some time, be prepared\n",
    "fit_neural_net(hidden_layer_sizes=400, alpha=0.2, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 190556.97147976\n",
      "Iteration 2, loss = 292002.47158714\n",
      "Iteration 3, loss = 292794.39801545\n",
      "Iteration 4, loss = 292783.92279442\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Training set accuracy: 10.70%\n",
      "Test set accuracy: 10.25%\n"
     ]
    }
   ],
   "source": [
    "# You can safely ignore any warnings about convergence,\n",
    "# all they tell you is that maybe you should train for more iterations\n",
    "fit_neural_net(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "               solver='sgd', verbose=10, tol=1e-5, \n",
    "               random_state=1, learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. Quite a few parameters to play around! In your Machine Learning endeavors you probably need to search for these parameters via cross-validation, grid searches, or randomly (*which is not the same as what we did here*), but these are more advanced topics.\n",
    "\n",
    "Anyhow, we managed to get close to 100% training accuracy, and a test set accuracy of around ~91%, which is good but lower than what SVM achieved. Multi-Layer Perceptrons are cool, but very basic. Also, this it not the `digits` toy dataset we've used before, this is real deal.\n",
    "\n",
    "Now, let's see how one of the more modern algorithms, a Convolutional Neural Network (or CNN, or ConvNets) model, performs. These are precisely built to work well with images. We'll use (just for the sake of demonstration) one of the more advanced ConvNets, `MobileNet`, that was designed for use in devices with lower computational power (it still consumes significant resources, but it much lighter than other Deep Neural Networks). Before that, though, we need to reshape our data to be in 3 dimensions (width, height, channels) since that is what a CNN usually expects (ConvNets, unlike other algorithms, are designed to work with images!). We also need to change our Y variables to categorical vectors (similar to what we used for text data; we use [keras' to_categorical](https://keras.io/utils/#to_categorical)). That said, we won't explain the parameters used here, or how CNNs work, that deserves a whole *course* on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates a vector of length `num_classes` with the X element being equal to 1 and all the others 0. Here are a few examples so you can see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting from 0\n",
    "to_categorical(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(3, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(3, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((-1,28,28,1))\n",
    "x_test = x_test.reshape((-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 15s 2ms/step - loss: 0.8145 - acc: 0.7232 - val_loss: 1.3888 - val_acc: 0.6800\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2401 - acc: 0.9220 - val_loss: 0.7905 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1495 - acc: 0.9519 - val_loss: 0.2762 - val_acc: 0.9260\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0992 - acc: 0.9726 - val_loss: 0.2296 - val_acc: 0.9430\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0886 - acc: 0.9721 - val_loss: 0.3551 - val_acc: 0.9200\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0742 - acc: 0.9759 - val_loss: 0.1663 - val_acc: 0.9610\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0617 - acc: 0.9810 - val_loss: 0.1913 - val_acc: 0.9560\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0569 - acc: 0.9830 - val_loss: 0.1511 - val_acc: 0.9570\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0756 - acc: 0.9761 - val_loss: 0.1807 - val_acc: 0.9640\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0618 - acc: 0.9820 - val_loss: 0.1762 - val_acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20dee067f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# we need to upsample the images to use this model\n",
    "model.add(UpSampling2D(size=(2,2), input_shape=(28,28,1)))\n",
    "\n",
    "mobilenet = MobileNet(weights=None, classes=10, input_shape=(56,56,1))\n",
    "model.add(mobilenet)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# This will take a LONG time to train if you don't have a GPU\n",
    "model.fit(x_train, y_train, epochs=10, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained for only 10 epochs, and with virtually no parameter tuning so the results are quite good (~96% validation -and test (see below)- accuracy)! One thing that we would like to note here is that we used a `validation_split` parameter. This is essentially the same as using a test set, but we monitor it during training. Often, we tune our models based on how the model performs on the test data and this behavior causes \"test set overfitting\". We will instead use the actual test data only at the end, and tune the model based on the validation set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 228us/step\n",
      "Test set accuracy: 96.20%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test set accuracy: {:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we covered just a very rough outline and we used off-the-shelf models without getting into any specifics. It is worth noting that the best models on MNIST are getting over 99.7% accuracy (with SVM's achieving ~99.5%)! Feel free to play around, and take a look at [this Wikipedia table](https://en.wikipedia.org/wiki/MNIST_database#Classifiers).\n",
    "\n",
    "The field of Neural Networks is quite broad and covering it in one short article is impossible, but we saw them in practice. We hope that this fueled your interest to dig deeper into the fields of Machine Learning and A.I.!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
